
CondaError: Run 'conda init' before 'conda activate'

2025-03-20 01:51:22,297	INFO worker.py:1812 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:02<00:18,  2.35s/it]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:04<00:16,  2.38s/it]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:07<00:14,  2.48s/it]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:10<00:13,  2.67s/it]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:12<00:09,  2.38s/it]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:14<00:07,  2.36s/it]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:17<00:05,  2.52s/it]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:19<00:02,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:20<00:00,  1.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:20<00:00,  2.27s/it]

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:01<00:47,  1.41s/it]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:02<00:39,  1.18s/it]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:03<00:34,  1.08s/it]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:04<00:31,  1.02s/it]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:05<00:29,  1.03it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:06<00:27,  1.04it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:07<00:27,  1.03it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:08<00:25,  1.06it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:08<00:23,  1.10it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:09<00:22,  1.12it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:10<00:21,  1.13it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:11<00:20,  1.13it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:12<00:22,  1.02s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:13<00:21,  1.00s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:15<00:22,  1.11s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:16<00:21,  1.14s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:17<00:20,  1.12s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:18<00:18,  1.10s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:19<00:16,  1.06s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:20<00:15,  1.05s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:21<00:14,  1.03s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:22<00:13,  1.02s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:23<00:12,  1.01s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:24<00:11,  1.03s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:25<00:10,  1.02s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:26<00:09,  1.00s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:27<00:07,  1.00it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:28<00:06,  1.02it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:29<00:05,  1.04it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:30<00:04,  1.03it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:30<00:03,  1.13it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:31<00:02,  1.21it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:32<00:01,  1.31it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:32<00:00,  1.34it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:33<00:00,  1.22it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:33<00:00,  1.03it/s]
2025-03-20 01:53:25,688 - INFO - Loaded model from hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4. - CPU Memory usage: 7385.91 MB RSS
2025-03-20 01:53:25,725 - INFO - GPU 0: 72631/81920 MB
2025-03-20 01:53:25,726 - INFO - GPU 1: 72631/81920 MB
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/pi_miyyer_umass_edu/anmol/oneruler/OneRuler/ruler.py", line 121, in <module>
[rank0]:     process_data_dump(args.model_short_name, input_file_path, args.ctx, out_file_path, llm, tokenizer)
[rank0]:   File "/work/pi_miyyer_umass_edu/anmol/oneruler/OneRuler/ruler.py", line 23, in process_data_dump
[rank0]:     with open(input_file, 'r', encoding='utf-8') as f:
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/work/pi_miyyer_umass_edu/yekyung/git/Multilang-RULER/scripts/benchmark_root/Llama3.1/pl/8000/data/niah_single/validation.jsonl'
[rank0]:[W320 01:53:30.418664915 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/work/pi_miyyer_umass_edu/anmol/conda_storage/.conda/envs/helmet/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/work/pi_miyyer_umass_edu/anmol/conda_storage/.conda/envs/helmet/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
